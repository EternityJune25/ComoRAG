<h1 align="center">ComoRAG</h1>
<div align="center">

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/) [![CUDA](https://img.shields.io/badge/CUDA-12.x-green)](https://developer.nvidia.com/cuda-zone) [![Platform](https://img.shields.io/badge/Platform-Linux-lightgrey)](https://kernel.org/) [![RAG](https://img.shields.io/badge/RAG-Graph%20Memory-orange)](#é¡¹ç›®ä»‹ç») [![LLM](https://img.shields.io/badge/LLM-OpenAI%2FvLLM-purple)](#ä¸»è¦æ¨¡å—) [![Status](https://img.shields.io/badge/Status-Active-success)](#) [![arXiv](https://img.shields.io/badge/arXiv-2508.10419-b31b1b.svg)](https://arxiv.org/abs/2508.10419) [![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE) [![DeepWiki](https://img.shields.io/badge/DeepWiki-ComoRAG-purple)](https://deepwiki.com/EternityJune25/ComoRAG)

[English](README.md) | [ä¸­æ–‡](README_zh.md)

</div>

<p align="center">
  <img src="assert/img/overview.png" alt="ComoRAG Overview" width="100%">
</p>

## ğŸ“– è®ºæ–‡ä¿¡æ¯

è¿™æ˜¯ä»¥ä¸‹è®ºæ–‡çš„**å®˜æ–¹å®ç°**ï¼š

**[ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)**


**å¼•ç”¨ï¼š**
```bibtex
@article{wang2025comorag,
  title={ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning},
  author={Wang, Juyuan and Zhao, Rongchen and Wei, Wei and Wang, Yufeng and Yu, Mo and Zhou, Jie and Xu, Jin and Xu, Liyan},
  journal={arXiv preprint arXiv:2508.10419},
  year={2025}
}
```

---

## é¡¹ç›®ä»‹ç»
ComoRAGæ˜¯ä¸€ä¸ªç”¨äºé•¿æ–‡æ¡£å’Œå¤šæ–‡æ¡£é—®ç­”ã€ä¿¡æ¯æå–å’ŒçŸ¥è¯†å›¾è°±æ„å»ºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿã€‚å®ƒé›†æˆäº†å„ç§LLMã€åµŒå…¥æ¨¡å‹ã€åŸºäºå›¾çš„æ¨ç†å’Œè¯„ä¼°å·¥å…·ï¼Œé€‚ç”¨äºç ”ç©¶å’Œå®é™…åº”ç”¨ã€‚

ğŸ”¥ ComoRAGæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ

ç”±äºå¤æ‚çš„æ•…äº‹æƒ…èŠ‚å’Œä¸æ–­æ¼”åŒ–çš„è§’è‰²/å®ä½“å…³ç³»ï¼Œé•¿æ•…äº‹å’Œå°è¯´ä¸­çš„å™äº‹ç†è§£å¾ˆå›°éš¾ã€‚LLMåœ¨æ‰©å±•ä¸Šä¸‹æ–‡å’Œæˆæœ¬æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› æ­¤æ£€ç´¢ä»ç„¶è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç»å…¸çš„RAGé€šå¸¸æ˜¯æ— çŠ¶æ€å’Œå•æ­¥çš„ï¼Œé”™è¿‡äº†é•¿è·ç¦»ã€ç›¸äº’å…³è”æ¨ç†çš„åŠ¨æ€æ€§è´¨ã€‚

ComoRAGé‡‡ç”¨è®¤çŸ¥å¯å‘çš„æ–¹æ³•ï¼šå™äº‹æ¨ç†ä¸æ˜¯ä¸€æ¬¡æ€§çš„ï¼Œè€Œæ˜¯æ–°è¯æ®è·å–å’Œè¿‡å»çŸ¥è¯†æ•´åˆä¹‹é—´çš„åŠ¨æ€ã€æ¼”åŒ–çš„ç›¸äº’ä½œç”¨â€”â€”ç±»ä¼¼äºå¤§è„‘ä¸­è®°å¿†è¿‡ç¨‹çš„ç±»æ¯”ã€‚ğŸ§ 

- ğŸ” è¿­ä»£æ¨ç†å¾ªç¯ï¼šå½“é‡åˆ°æ¨ç†éšœç¢æ—¶ï¼ŒComoRAGå¯åŠ¨ä¸åŠ¨æ€è®°å¿†å·¥ä½œç©ºé—´äº¤äº’çš„å¾ªç¯ã€‚
- ğŸ•µï¸ æ¢æµ‹æŸ¥è¯¢ï¼šæ¯ä¸ªå¾ªç¯ç”Ÿæˆæœ‰é’ˆå¯¹æ€§çš„æ¢æµ‹ï¼Œä»¥æ¢ç´¢æ–°çš„è¯æ®è·¯å¾„ã€‚
- ğŸ§³ å…¨å±€è®°å¿†æ± ï¼šæ–°æ£€ç´¢çš„è¯æ®è¢«æ•´åˆåˆ°å…±äº«è®°å¿†æ± ä¸­ï¼Œé€æ­¥ä¸ºæŸ¥è¯¢æ„å»ºè¿è´¯çš„ä¸Šä¸‹æ–‡ã€‚

ğŸš€ åŸºå‡†æµ‹è¯•å’Œæ”¶ç›Šï¼šåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿ä¸Šä¸‹æ–‡å™äº‹åŸºå‡†æµ‹è¯•ï¼ˆ200K+ tokensï¼‰ä¸Šï¼ŒComoRAGä¼˜äºå¼ºå¤§çš„RAGåŸºçº¿ï¼Œä¸æœ€å¼ºåŸºçº¿ç›¸æ¯”ï¼Œç›¸å¯¹æ”¶ç›Šé«˜è¾¾11%ã€‚å®ƒåœ¨éœ€è¦å…¨å±€ç†è§£çš„å¤æ‚æŸ¥è¯¢ä¸Šç‰¹åˆ«å‡ºè‰²ï¼Œä¸ºåŸºäºæ£€ç´¢çš„é•¿ä¸Šä¸‹æ–‡ç†è§£æä¾›äº†åŸåˆ™æ€§çš„ã€è®¤çŸ¥åŠ¨æœºçš„ã€æœ‰çŠ¶æ€çš„æ¨ç†ã€‚ğŸ“ˆ

ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒæ€æƒ³ï¼šæ¨ç† â†’ æ¢æµ‹ â†’ æ£€ç´¢ â†’ æ•´åˆ â†’ è§£å†³ã€‚ğŸ§©

---

## ä¸»è¦ç‰¹æ€§ âœ¨
- ğŸ§  æ”¯æŒå¤šç§ LLM åŠæœ¬åœ°/è¿œç¨‹åµŒå…¥æ¨¡å‹
- ğŸ•¸ï¸ å›¾ç»“æ„å¢å¼ºçš„æ£€ç´¢ä¸æ¨ç†
- ğŸ”§ çµæ´»çš„æ•°æ®é¢„å¤„ç†ä¸åˆ†å—
- ğŸ“Š å¤šç§è¯„æµ‹æŒ‡æ ‡ï¼ˆEMã€F1 ç­‰ï¼‰
- ğŸ§± æ¨¡å—åŒ–ã€æ˜“æ‰©å±•è®¾è®¡

---

## ç›®å½•ç»“æ„ ğŸ“‚
```
ComoRAG/
â”œâ”€â”€ main_openai.py                       # ä½¿ç”¨ OpenAI API çš„ä¸»ç¨‹åº
â”œâ”€â”€ main_vllm.py                         # ä½¿ç”¨æœ¬åœ° vLLM æœåŠ¡å™¨çš„ä¸»ç¨‹åº
â”œâ”€â”€ script/                              # æ•°æ®å¤„ç†ä¸è¯„æµ‹è„šæœ¬
â”‚   â”œâ”€â”€ chunk_doc_corpus.py              # æ–‡æ¡£åˆ†å—è„šæœ¬
â”‚   â””â”€â”€ eval_qa.py                       # é—®ç­”è¯„æµ‹è„šæœ¬
â”œâ”€â”€ dataset/                             # æ•°æ®é›†ç›®å½•
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/comorag/                        # æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ ComoRAG.py                       # ä¸»ç±»ä¸æ ¸å¿ƒé€»è¾‘
â”‚   â”œâ”€â”€ utils/                           # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ embedding_model/                 # åµŒå…¥æ¨¡å‹ç›¸å…³
â”‚   â”œâ”€â”€ llm/                             # LLM ç›¸å…³
â”‚   â”œâ”€â”€ prompts/                         # æç¤ºè¯æ¨¡æ¿
â”‚   â”œâ”€â”€ information_extraction/          # ä¿¡æ¯æŠ½å–
â”‚   â””â”€â”€ rerank.py, embedding_store.py    # å…¶ä»–æ ¸å¿ƒæ¨¡å—
â”œâ”€â”€ requirements.txt                     # ä¾èµ–åŒ…
â””â”€â”€ README.md / README_zh.md             # é¡¹ç›®è¯´æ˜
```

---

## å®‰è£…ä¸ç¯å¢ƒé…ç½® ğŸ› ï¸
1. ğŸ **Python ç‰ˆæœ¬**ï¼šå»ºè®® Python 3.10 åŠä»¥ä¸Š
2. ğŸ“¦ **ä¾èµ–å®‰è£…**ï¼š
```bash
pip install -r requirements.txt
```
3. ğŸ”‘ **ç¯å¢ƒå˜é‡**ï¼šæ ¹æ®éœ€è¦è®¾ç½® OpenAI API Key æˆ–æœ¬åœ° LLM/åµŒå…¥æ¨¡å‹è·¯å¾„
4. âš™ï¸ **GPUï¼ˆå¯é€‰ä½†æ¨èï¼‰**ï¼šrequirements.txt ä¸­å¤šé¡¹ä¾èµ–æ”¯æŒ CUDA 12.x

---

## æ•°æ®å‡†å¤‡ä¸æ ¼å¼ ğŸ“„
- ğŸ“š **è¯­æ–™æ–‡ä»¶ corpus.jsonl**ï¼šæ¯è¡Œä¸€ä¸ªæ–‡æ¡£ï¼Œå­—æ®µå¦‚ `id`, `doc_id`, `title`, `contents`
- â“ **é—®ç­”æ–‡ä»¶ qas.jsonl**ï¼šæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼Œå­—æ®µå¦‚ `id`, `question`, `golden_answers`

ç¤ºä¾‹ï¼š

corpus.jsonl:
```json
{"id": 0, "doc_id": 1, "title": "...", "contents": "..."}
```
qas.jsonl:
```json
{"id": "1", "question": "...", "golden_answers": ["..."]}
```

---

## å¿«é€Ÿå¼€å§‹ âš¡

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ OpenAI APIï¼ˆmain_openai.pyï¼‰ğŸš€

1. åœ¨è„šæœ¬ä¸­é…ç½®æ•°æ®é›†è·¯å¾„å’Œæ¨¡å‹å‚æ•°
```python
config = BaseConfig(
    llm_base_url='https://api.example.com/v1',  # OpenAI API
    llm_name='gpt-4o-mini',
    dataset='cinderella',
    embedding_model_name='/path/to/your/embedding/model',
    embedding_batch_size=32,
    need_cluster=True,  # å¯ç”¨è¯­ä¹‰/æƒ…èŠ‚å¢å¼º
    output_dir='result/cinderella',
    save_dir='outputs/cinderella',
    max_meta_loop_max_iterations=5,  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    is_mc=False,  # æ˜¯å¦é€‰æ‹©é¢˜
    max_tokens_ver=2000,  # éªŒè¯å±‚æœ€å¤§tokenæ•°
    max_tokens_sem=2000,  # è¯­ä¹‰å±‚æœ€å¤§tokenæ•°
    max_tokens_epi=2000   # æƒ…èŠ‚å±‚æœ€å¤§tokenæ•°
)
```
2. è¿è¡Œä¸»ç¨‹åº â–¶ï¸ï¼š
```bash
python main_openai.py
```

### æ–¹æ³•äºŒï¼šä½¿ç”¨æœ¬åœ° vLLM æœåŠ¡å™¨ï¼ˆmain_vllm.pyï¼‰âš¡

#### 1. å¯åŠ¨ vLLM æœåŠ¡å™¨ ğŸš€

é¦–å…ˆå¯åŠ¨ vLLM OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨ï¼š

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨ vllm serve å‘½ä»¤
vllm serve /path/to/your/model \
  --tensor-parallel-size 1 \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.95

# æ–¹å¼äºŒï¼šä½¿ç”¨ python -m vllm.entrypoints.openai.api_server
python -m vllm.entrypoints.openai.api_server \
  --model /path/to/your/model \
  --served-model-name your-model-name \
  --tensor-parallel-size 1 \
  --max-model-len 32768 \
  --dtype auto
```

**å‚æ•°è¯´æ˜ï¼š**
- `--model`ï¼šæ¨¡å‹è·¯å¾„ï¼ˆå¦‚ `/path/to/your/model`ï¼‰
- `--tensor-parallel-size`ï¼šGPU å¹¶è¡Œæ•°é‡
- `--max-model-len`ï¼šæœ€å¤§æ¨¡å‹é•¿åº¦
- `--gpu-memory-utilization`ï¼šGPU å†…å­˜ä½¿ç”¨ç‡

#### 2. é…ç½® main_vllm.py ğŸ“

ä¿®æ”¹ `main_vllm.py` ä¸­çš„é…ç½®ï¼š

```python
# vLLM æœåŠ¡å™¨é…ç½®
vllm_base_url = 'http://localhost:8000/v1'  # vLLM æœåŠ¡å™¨åœ°å€
served_model_name = '/path/to/your/model'    # æ¨¡å‹è·¯å¾„

config = BaseConfig(
    llm_base_url=vllm_base_url,
    llm_name=served_model_name,
    llm_api_key="your-api-key-here",  # ä»»æ„å€¼ï¼Œæœ¬åœ°æœåŠ¡å™¨ä¸éœ€è¦çœŸå® API key
    dataset='cinderella',
    embedding_model_name='/path/to/your/embedding/model',
    embedding_batch_size=4,
    need_cluster=True,
    output_dir='result/cinderella_vllm',
    save_dir='outputs/cinderella_vllm',
    max_meta_loop_max_iterations=5,
    is_mc=False,
    max_tokens_ver=2000,
    max_tokens_sem=2000,
    max_tokens_epi=2000
)
```

#### 3. è¿è¡Œç¨‹åº â–¶ï¸

```bash
python main_vllm.py
```

#### 4. æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€ ğŸ”

ç¡®ä¿ vLLM æœåŠ¡å™¨æ­£å¸¸è¿è¡Œï¼š

```bash
# æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
netstat -tlnp | grep 8000

# æµ‹è¯• API è¿æ¥
curl http://localhost:8000/v1/models
```

### ä¸¤ç§æ–¹æ³•çš„åŒºåˆ« ğŸ“Š

| ç‰¹æ€§ | OpenAI API (main_openai.py) | vLLM æœ¬åœ° (main_vllm.py) |
|------|---------------------|-------------------------|
| æˆæœ¬ | æŒ‰ token æ”¶è´¹ | ä¸€æ¬¡æ€§æ¨¡å‹ä¸‹è½½ |
| é€Ÿåº¦ | ç½‘ç»œå»¶è¿Ÿ | æœ¬åœ°æ¨ç†ï¼Œæ›´å¿« |
| éšç§ | æ•°æ®å‘é€åˆ°äº‘ç«¯ | å®Œå…¨æœ¬åœ°å¤„ç† |
| é…ç½® | ç®€å•ï¼Œåªéœ€ API key | éœ€è¦ GPU å’Œæ¨¡å‹æ–‡ä»¶ |
| ç¨³å®šæ€§ | ä¾èµ–ç½‘ç»œ | æœ¬åœ°æ§åˆ¶ |

3. ğŸ“ ç»“æœå°†ä¿å­˜åœ¨ `result/` ç›®å½•ä¸‹

---

## ä¸»è¦æ¨¡å—è¯´æ˜
- ğŸ›ï¸ `ComoRAG.py`ï¼šç³»ç»Ÿä¸»ç±»ï¼Œè´Ÿè´£æ£€ç´¢ã€æ„å»ºã€æ¨ç†ä¸é—®ç­”
- ğŸ§° `utils/`ï¼šé…ç½®ã€æ—¥å¿—ã€åµŒå…¥ã€èšç±»ã€æ‘˜è¦ã€è®°å¿†ã€æ™ºèƒ½ä½“ç­‰å·¥å…·
- ğŸ§² `embedding_model/`ï¼šåµŒå…¥æ¨¡å‹é€‚é…ä¸åŠ è½½
- ğŸ¤– `llm/`ï¼šå¤§è¯­è¨€æ¨¡å‹é€‚é…
- ğŸ—’ï¸ `prompts/`ï¼šæç¤ºè¯æ¨¡æ¿ç®¡ç†
- ğŸ“¦ `embedding_store.py`ï¼šåµŒå…¥å‘é‡å­˜å‚¨ä¸æ£€ç´¢

---

## æ•°æ®å¤„ç†ä¸è¯„æµ‹è„šæœ¬ ğŸ§ª
- âœ‚ï¸ `script/chunk_doc_corpus.py`ï¼šæ–‡æ¡£åˆ†å—ï¼Œæ”¯æŒæŒ‰ token/å¥å­/é€’å½’ç­‰æ–¹å¼
- ğŸ“ˆ `script/eval_qa.py`ï¼šè‡ªåŠ¨è¯„æµ‹é—®ç­”ç»“æœï¼Œæ”¯æŒ EMã€F1 ç­‰æŒ‡æ ‡

ä½¿ç”¨ç¤ºä¾‹ï¼š

æ–‡æ¡£åˆ†å— âœ‚ï¸ï¼š
```bash
python script/chunk_doc_corpus.py \
  --input_path dataset/<name>/<subset>/corpus.jsonl \
  --output_path dataset/<name>/<subset>/corpus_chunked.jsonl \
  --chunk_by token \
  --chunk_size 512 \
  --tokenizer_name_or_path /path/to/your/tokenizer
```

è¯„æµ‹é—®ç­”ç»“æœ ğŸ“Šï¼š
```bash
python script/eval_qa.py /path/to/result/<dataset>/<subset>
```
å°†ç”Ÿæˆ `details`ã€`results.json` ç­‰æ–‡ä»¶ã€‚

---

## è”ç³»ä¸è´¡çŒ® ğŸ¤
å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤ Issue æˆ– PRã€‚

---

## è‡´è°¢
æœ¬é¡¹ç›®å‚è€ƒäº† [HippoRAG](https://github.com/OSU-NLP-Group/HippoRAG) ä»“åº“çš„éª¨æ¶ä»£ç ã€‚

---

## Star History â­

[![Star History Chart](https://api.star-history.com/svg?repos=EternityJune25/ComoRAG&type=Date)](https://star-history.com/#OSU-NLP-Group/ComoRAG&Date)
